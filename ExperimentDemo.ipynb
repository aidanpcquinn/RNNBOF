{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "offensive-rating",
   "metadata": {},
   "source": [
    "# RNN-BOF Demonstration & Usage\n",
    "\n",
    "## Demo notebook for: Globally Trained Recurrent Neural Network for Forecastingthe Presence of Inpatient Aggression\n",
    "\n",
    "\n",
    "## Enviroment Details:\n",
    "\n",
    "### Docker info\n",
    "- nvcr.io/nvidia/tensorflow:21.03-tf2-py3\n",
    "- All experiments were performed in the oficial nvidia tensorflow docker container\n",
    "\n",
    "#### Specific versions of imported libraries can be seen in requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-indonesian",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blank-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing imports\n",
    "from processingUtilities import dataFormatting\n",
    "from processingUtilities import preProcessing\n",
    "from processingUtilities import windowingFunctions\n",
    "\n",
    "# modelling and experimental imports\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import make_scorer\n",
    "from evaluationUtilities import evalFunctions\n",
    "from evaluationUtilities import saveLoadResults\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from RNNBOF import RNNBOFKeras\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "import time\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# loading imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-affairs",
   "metadata": {},
   "source": [
    "## Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "little-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix seeds for tensor flow and numpy\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "rstate = np.random.RandomState(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-milan",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "protecting-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data in, formatted as pandas dataframe containing all entities, an id coulumn, and an ordering column\n",
    "# The label series/column must be numerical type with 1 representing positive\n",
    "# All other columns must be numerical (1 hot encoded for catagorical, non ordinal variables)\n",
    "\n",
    "# eg. fullDataFrame = pd.read_csv('________.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manyTimeToDict: Converts a global dataframe containing time stamps and ids into dict of dataframes per entity where keys are the entity IDs.\n",
    "    # Inputs:\n",
    "        # sortColumnName: the column to sort on (ascending) \n",
    "        # dropID: sepcifies if the id column should be dropped from individual dataframes\n",
    "        # dataFrame: input global dataframe (contains multiple entities entries over time)\n",
    "        # idColumn: the column entitiy dataframes are grouped on.\n",
    "    # Returns:\n",
    "        # dict of dataframes, with keys representing the entities IDs, ordered by the sorting column\n",
    "\n",
    "entityDict = dataFormatting.manyTimeToDict(idColumn = 'id', dataFrame = fullDataFrame, sortColumnName = 'sortColumnName', dropID = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-smile",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-genealogy",
   "metadata": {},
   "source": [
    "### Compressed days and missing value treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "divine-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unCompressor: where our assessments span more than 24 hours, multiple treatment options are possible\n",
    "    # Inputs:\n",
    "        # skipTreatment: Type of treatment applied to skipped days.\n",
    "            # skipTreament = \"None\": no treatment applied, returns frame as is\n",
    "            # skipTreament = \"Dupe\": skipped days are duped for the number of days skipped\n",
    "            # skipTreament = \"None\": skipped days are duped for the number of days skipped, but dynamic duped values are replaced with Nan to be interpolated\n",
    "        # dictOfTDFs: dictionary of dataframes for each entitiy\n",
    "    # Returns:\n",
    "        # dict of dataframes with treated skip days in 1 of three ways (skipTreament)\n",
    "        \n",
    "# NB: this preProcessing is only applicable in specific circumstances like the ones detailed in the paper.\n",
    "\n",
    "entityDict2 = preProcessing.unCompressor(dictOfTDFs = entityDict, skipTreatment='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "separate-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolator: interpolates dict of dataframes missing values in one of three ways\n",
    "    # Inputs:\n",
    "        # interpolationType: Type if interpolation applied to missing values\n",
    "            # interpolateType = \"None\": None interpolation applied\n",
    "            # interpolateType = \"Linear\": Linear interpolation applied\n",
    "            # interpolateType = \"Pad\": Padding interpolation applied\n",
    "        # dictOfTDFs: dictionary of dataframes for each entitiy\n",
    "    # Returns:\n",
    "        # dict of dataframes with interpolation performed as specifified.\n",
    "\n",
    "entityDict3 = preProcessing.interpolator(dictOfTDFs = entityDict2, interpolateType='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-consciousness",
   "metadata": {},
   "source": [
    "### Column type identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "decimal-diameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamic covariables: 73\n",
      "catagorical covariables: 40\n"
     ]
    }
   ],
   "source": [
    "#colTyper: takes dict of dataframes returns list of dynamic and catagorical columns and prints stats\n",
    "    # Inputs:\n",
    "        # dictOfTDFs: dictionary of dataframes for each entity\n",
    "    # Returns:\n",
    "        # names of collumns that change over time, and stay static, accross all entities, as two lists\n",
    "        \n",
    "dynamicCols, cataCols = preProcessing.colTyper(dictOfTDFs = entityDict3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-cycle",
   "metadata": {},
   "source": [
    "### Removing entities with short history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nuclear-pulse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 23 entities for being less than 30 periods long\n",
      "total remaining entities: 83\n"
     ]
    }
   ],
   "source": [
    "# shortIDRemover: Removes all entities with less than a certain amount of entries\n",
    "    # Inputs:\n",
    "        # dictOfTDFs: dictionary of dataframes for each entity\n",
    "        # shortStayLength: minimum length required per entity\n",
    "    # Returns:\n",
    "        # dict of dataframes with those shorter than shortStayLength removed\n",
    "    \n",
    "# NB: must be at shortest, 3 time stamps longer than than the longest input window length\n",
    "    \n",
    "entityDict4 = preProcessing.shortIDRemover(dictOfTDFs = entityDict3, shortStayLength = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-batch",
   "metadata": {},
   "source": [
    "### Data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "empirical-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizeOnTrain: standardize from the windows in the training set, apply to full dataframe.\n",
    "    # Inputs:\n",
    "        # dictOfTDFs: dictionary of dataframes for each entity\n",
    "        # testPercent: percent of ending windows heldout for each entity\n",
    "        # labelColumn: name of column containing the label binary values\n",
    "        # holdOutLength: Number of windows removed completly\n",
    "    # Returns:\n",
    "        # standardized entity dict\n",
    "        # dict of columns standard deviation across training data\n",
    "        # dict of columns mean across training data\n",
    "\n",
    "# NB: holdOutLength and testPercent must match the ones used in all following functions\n",
    "    \n",
    "standEntityDict, stdDict, meanDict = preProcessing.standardizeOnTrain(dictOfTDFs = entityDict4, testPercent = .2, labelColumn = 'anyAggression', holdOutLenght = 1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-thesaurus",
   "metadata": {},
   "source": [
    "## Windowing and Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-duplicate",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "better-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeSeriesPercentTrainTestSplit: splits the full dict of dataframes into two containing the relevent data to be windowed by train and test windower.\n",
    "    # Inputs:\n",
    "        # dictOfTDFs: dictionary of dataframes for each entity\n",
    "        # testPercent: percent of ending windows heldout for each entity\n",
    "        # holdOutLength: Number of windows removed completly\n",
    "        # MaxInputWindow: input window length\n",
    "    # Returns:\n",
    "        # Dict of dataframes per entity for train and test sets\n",
    "\n",
    "# NB: this is the split process documented in the accompanying paper (Section 3.3) and the test dict contains the windowed days into the train set - the holdout length\n",
    "    \n",
    "trainDict10, testDict10 = preProcessing.timeSeriesPercentTrainTestSplit(dictOfTDFs = standEntityDict, testPercent = .2, MaxInputWindow = 10 , holdOutLength = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-hebrew",
   "metadata": {},
   "source": [
    "### Train and test folds (Dataframe version, used by sklearn and LightGBM models and FFNN in keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "played-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainWindowerRolling: returns the rolling training windows as described in section 3.3 and crossvalidation fold data, using coulumn names to represent sequential data\n",
    "    # Inputs:\n",
    "        # trainingDfDict: training data from timeSeriesPercentTrainTestSplit as a dict of dataframes\n",
    "        # numberOfFolds: number of folds for sequential cross validation (section 3.3 and 4.4). will create numberOfFolds + 1 pools for numberOfFolds sequential evaluations\n",
    "        # windowLength: input window length\n",
    "        # dynamicCols: dynamic column names list as created by colTyper\n",
    "        # cataCols: static column names list as created by colTyper\n",
    "        # predictionVariable: name of column containing the label binary values\n",
    "    # Returns:\n",
    "        # dataframe of training windows, with columns named: name_lag_0, name_lag_1 ....  with the number representing how many iterations back in time that variable represents\n",
    "        # np.array of the labels corresponding to each window in the dataframe\n",
    "        # array of arrays to be used by the custom cross validation function, containg the DF index for each cross validations folds\n",
    "        \n",
    "# NB: this function needs to be optimised, it currently is extermely slow.\n",
    "# NB: This is the datafram/flat version as used by all non iterative models such as ours.\n",
    "\n",
    "x_train10, y_train10, foldIdx10 = windowingFunctions.trainWindowerRolling(trainingDfDict = trainDict10, numberOfFolds = 5, windowLength = 10, dynamicCols = dynamicCols, cataCols = cataCols, predictionVariable = 'anyAggression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "graduate-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testWindower: returns the rolling trainign windows as described in section 3.3\n",
    "    # Inputs:\n",
    "        # testingDfDict: testing data from timeSeriesPercentTrainTestSplit as a dict of dataframes\n",
    "        # windowLength: input window length\n",
    "        # dynamicCols: dynamic column names list as created by colTyper\n",
    "        # cataCols: static column names list as created by colTyper\n",
    "        # predictionVariable: name of column containing the label binary values\n",
    "    # Returns:\n",
    "        # dataframe of testing windows, with columns named: name_lag_0, name_lag_1 ....  with the number representing how many iterations back in time that variable represents\n",
    "        # np.array of the labels corresponding to each window in the dataframe\n",
    "        \n",
    "# NB: this function needs to be optimised, it currently is extermely slow.\n",
    "\n",
    "x_test10, y_test10 = windowingFunctions.testWindower(testingDfDict = testDict10, windowLength = 10, dynamicCols = dynamicCols, cataCols = cataCols, predictionVariable = 'anyAggression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fixed-dependence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 10 shape:\n",
      "x train shape: (8513, 780)\n",
      "x test shape: (2213, 780)\n"
     ]
    }
   ],
   "source": [
    "print('DataFrame 10 shape:')\n",
    "print('x train shape:', x_train10.shape)\n",
    "print('x test shape:', x_test10.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-filename",
   "metadata": {},
   "source": [
    "### Train and test folds (3D Arrays version, used by RNN-BOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "informative-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrayTrainWindowerRolling: returns the rolling trainign windows as described in section 3.3 and crossvalidation fold data, using arrays to represent sequential windows\n",
    "    # Inputs:\n",
    "        # trainingDfDict: training data from timeSeriesPercentTrainTestSplit as a dict of dataframes\n",
    "        # numberOfFolds: number of folds for sequential cross validation (section 3.3 and 4.4). will create numberOfFolds + 1 pools for numberOfFolds sequential evaluations\n",
    "        # windowLength: input window length\n",
    "        # predictionVariable: name of column containing the label binary values\n",
    "        # timeFirst: indication for if window arrays should be time or feature first\n",
    "            # timeFirst = True: output.shape = (window_number, timestep, feature)\n",
    "            # timeFirst = False: output.shape = (window_number, feature, timestep)\n",
    "            # True is used by RNN-BOF\n",
    "        # sliding: indication for if windows should be sliding or non-overlapping\n",
    "            # True is used by RNN-BOF\n",
    "        # Univariate: Use True if data is univariate\n",
    "        # removeNaNY: If true, removes windows and labels if the label is missing\n",
    "            # redundant due to other pre-processing steps\n",
    "    # Returns:\n",
    "        # 3D np.array representing windows, timesteps and features\n",
    "        # np.array of the labels corresponding to each window in the dataframes\n",
    "        # array of arrays to be used by the custom cross validation function, containg the DF index for each cross validations folds\n",
    "\n",
    "ax_train10, ay_train10, afoldIdx10 = windowingFunctions.arrayTrainWindowerRolling(trainingDfDict = trainDict10, numberOfFolds = 5, windowLength =  10, predictionVariable = 'anyAggression', \n",
    "                                                                            timeFirst = True, sliding = True, univariate = False, removeNaNY = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "floral-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrayTestWindower: returns the rolling testing windows as described in section 3.3 and crossvalidation fold data, using arrays to represent sequential windows\n",
    "    # Inputs:\n",
    "        # testingDfDict: testing data from timeSeriesPercentTrainTestSplit as a dict of dataframes\n",
    "        # windowLength: input window length\n",
    "        # predictionVariable: name of column containing the label binary values\n",
    "        # timeFirst: indication for if window arrays should be time or feature first\n",
    "            # timeFirst = True: output.shape = (window_number, timestep, feature)\n",
    "            # timeFirst = False: output.shape = (window_number, feature, timestep)\n",
    "            # True is used by RNN-BOF\n",
    "        # sliding: indication for if windows should be sliding or non-overlapping\n",
    "            # True is used by RNN-BOF\n",
    "        # Univariate: Use True if data is univariate\n",
    "        # removeNaNY: If true, removes windows and labels if the label is missing\n",
    "            # redundant due to other pre-processing steps\n",
    "    # Returns:\n",
    "        # 3D np.array representing windows, timesteps and features\n",
    "        # np.array of the labels corresponding to each window in the dataframe\n",
    "\n",
    "ax_test10, ay_test10 = windowingFunctions.arrayTestWindower(testingDfDict = testDict10, windowLength = 10, predictionVariable =  'anyAggression', \n",
    "                                                      timeFirst = True, sliding = True, univariate =  False, removeNaNY =  True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "permanent-stylus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array 10 shape:\n",
      "train shape: (8513, 10, 114)\n",
      "test shape: (2213, 10, 114)\n"
     ]
    }
   ],
   "source": [
    "print('Array 10 shape:')\n",
    "print('train shape:', ax_train10.shape)\n",
    "print('test shape:', ax_test10.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-adolescent",
   "metadata": {},
   "source": [
    "## RNN-BOF experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-vatican",
   "metadata": {},
   "source": [
    "### Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "western-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comput class wieghts on the training set\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(class_weight = 'balanced', classes = np.asarray([0,1]), y = ay_train10)\n",
    "class_weightDict = {0: class_weights[0],\n",
    "                1: class_weights[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "confidential-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scoring function\n",
    "# we use area under precision recall curve\n",
    "\n",
    "auprg_scorer = make_scorer(evalFunctions.auprg, greater_is_better = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sacred-citation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputShape = (10, 114)\n"
     ]
    }
   ],
   "source": [
    "# defining set variables used across\n",
    "\n",
    "# input shape of a single window for a single entity.\n",
    "inputShape = ax_train10.shape[1:3]\n",
    "print('inputShape = ' + str(inputShape))\n",
    "\n",
    "# batch size used for training model\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-reynolds",
   "metadata": {},
   "source": [
    "### Hyperopt Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "medieval-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# use line magic to suprress outputs to stop jupyter notebook crashing\n",
    "# can be removed for small amounts of iterations/folds/epochs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    \n",
    "    # define lambda build function, from the rnnbof keras model\n",
    "    # takes params identified in the param space\n",
    "    # ranges can be defined below\n",
    "    build_func = lambda: RNNBOFKeras.get_lstm(shape = inputShape, \n",
    "                                              numLayers = params['number_of_layers'], \n",
    "                                              dropout = params['dropout'], \n",
    "                                              l2Val = params['l2'], \n",
    "                                              learnRate = 0.0001, \n",
    "                                              numHiddenNodes = params['hidden_nodes'])\n",
    "    \n",
    "    # build specific params\n",
    "    buildParams = {\n",
    "        'epochs': int(params['epochs']),\n",
    "        'batch_size': batch_size}\n",
    "    \n",
    "    # build sklearn wrapper for hyperopt tuning\n",
    "    rnnBofModel = KerasRegressor(build_fn = build_func, \n",
    "                                 verbose = -1, \n",
    "                                 **buildParams)\n",
    "    \n",
    "    # calculate score from built model using cros_val_score\n",
    "    # uses the custom cross validation function definied in windowingFunctions\n",
    "    # uses the auprg_scorer using https://github.com/meeliskull/prg implimentation\n",
    "    score = -(cross_val_score(rnnBofModel, \n",
    "                              ax_train10, \n",
    "                              ay_train10, \n",
    "                              scoring = auprg_scorer, \n",
    "                              fit_params = {'class_weight':class_weightDict}, \n",
    "                              cv = windowingFunctions.custom_idx_folder_rolling(afoldIdx10),\n",
    "                              verbose = 0).mean()\n",
    "             )\n",
    "    \n",
    "    return(score)\n",
    "    \n",
    "toc = time.time()\n",
    "\n",
    "tune_time_taken = tic-toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "verified-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined param space used in experiments\n",
    "paramSpace = {   \n",
    "     'epochs': hp.quniform('epochs', 10, 100, 1),\n",
    "     'hidden_nodes': hp.quniform('hidden_nodes', 10, 100, 1),\n",
    "     'number_of_layers' : hp.quniform('number_of_layers', 1,4,1),\n",
    "     'dropout': hp.quniform('dropout',0.1,0.5, 0.01),\n",
    "     'l2': hp.quniform('l2', 0.000001, 0.001, 0.000001)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns best hyperParams\n",
    "best = fmin(\n",
    "    fn = objective,\n",
    "    space = paramSpace,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = 100,\n",
    "    rstate = rstate\n",
    ")    \n",
    "# NB: returns some variables as wrong type: eg int for number_of_layers as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "level-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best identified during the 100 iterations of hyperopt ran for the experiment in the paper\n",
    "best = {'dropout': 0.5,\n",
    " 'epochs': 87.0,\n",
    " 'hidden_nodes': 21.0,\n",
    " 'l2': 0.000878,\n",
    " 'number_of_layers': 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-hospital",
   "metadata": {},
   "source": [
    "### Final train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "coated-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final RNNBOF keras model based on the best identified parameters\n",
    "finalRNNBOF10 = RNNBOFKeras.get_lstm(shape = inputShape, \n",
    "                                      numLayers = best['number_of_layers'], \n",
    "                                      dropout = best['dropout'], \n",
    "                                      l2Val = best['l2'], \n",
    "                                      learnRate = 0.0001, \n",
    "                                      numHiddenNodes = best['hidden_nodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "increasing-diploma",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/87\n",
      "17/17 [==============================] - 2s 3ms/step - loss: 0.8537\n",
      "Epoch 2/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.8387\n",
      "Epoch 3/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.8109\n",
      "Epoch 4/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.7663\n",
      "Epoch 5/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.7600\n",
      "Epoch 6/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.7273\n",
      "Epoch 7/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.7201\n",
      "Epoch 8/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.7114\n",
      "Epoch 9/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6768\n",
      "Epoch 10/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6993\n",
      "Epoch 11/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6848\n",
      "Epoch 12/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6484\n",
      "Epoch 13/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6739\n",
      "Epoch 14/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6391\n",
      "Epoch 15/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6459\n",
      "Epoch 16/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6446\n",
      "Epoch 17/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6360\n",
      "Epoch 18/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6127\n",
      "Epoch 19/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6230\n",
      "Epoch 20/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6183\n",
      "Epoch 21/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6406\n",
      "Epoch 22/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6139\n",
      "Epoch 23/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6179\n",
      "Epoch 24/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5905\n",
      "Epoch 25/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6049\n",
      "Epoch 26/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5851\n",
      "Epoch 27/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6106\n",
      "Epoch 28/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5897\n",
      "Epoch 29/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5937\n",
      "Epoch 30/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6044\n",
      "Epoch 31/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6032\n",
      "Epoch 32/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5925\n",
      "Epoch 33/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5724\n",
      "Epoch 34/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5951\n",
      "Epoch 35/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5839\n",
      "Epoch 36/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5654\n",
      "Epoch 37/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5863\n",
      "Epoch 38/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5828\n",
      "Epoch 39/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6049\n",
      "Epoch 40/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6077\n",
      "Epoch 41/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5718\n",
      "Epoch 42/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5610\n",
      "Epoch 43/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5511\n",
      "Epoch 44/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5731\n",
      "Epoch 45/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5647\n",
      "Epoch 46/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5439\n",
      "Epoch 47/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5639\n",
      "Epoch 48/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5506\n",
      "Epoch 49/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5533\n",
      "Epoch 50/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5550\n",
      "Epoch 51/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5681\n",
      "Epoch 52/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5515\n",
      "Epoch 53/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5650\n",
      "Epoch 54/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5687\n",
      "Epoch 55/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5405\n",
      "Epoch 56/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5325\n",
      "Epoch 57/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5363\n",
      "Epoch 58/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5504\n",
      "Epoch 59/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5393\n",
      "Epoch 60/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5369\n",
      "Epoch 61/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5249\n",
      "Epoch 62/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5312\n",
      "Epoch 63/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5333\n",
      "Epoch 64/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5609\n",
      "Epoch 65/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5501\n",
      "Epoch 66/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5497\n",
      "Epoch 67/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5303\n",
      "Epoch 68/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5297\n",
      "Epoch 69/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5351\n",
      "Epoch 70/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5348\n",
      "Epoch 71/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5193\n",
      "Epoch 72/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5171\n",
      "Epoch 73/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5291\n",
      "Epoch 74/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5201\n",
      "Epoch 75/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5358\n",
      "Epoch 76/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5182\n",
      "Epoch 77/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5364\n",
      "Epoch 78/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5216\n",
      "Epoch 79/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5159\n",
      "Epoch 80/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5162\n",
      "Epoch 81/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4974\n",
      "Epoch 82/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5266\n",
      "Epoch 83/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5143\n",
      "Epoch 84/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5080\n",
      "Epoch 85/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5351\n",
      "Epoch 86/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5398\n",
      "Epoch 87/87\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5309\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "# Final RNNBOF train, based on the best identified parameters\n",
    "history = finalRNNBOF10.fit(\n",
    "        ax_train10,\n",
    "        ay_train10,\n",
    "        epochs=int(best['epochs']),\n",
    "        batch_size = 512,\n",
    "        class_weight=class_weightDict\n",
    ")\n",
    "\n",
    "toc = time.time()\n",
    "final_train_time_taken = toc - tic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-sunday",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "alpine-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the held out test set\n",
    "\n",
    "probabilitiesPred = finalRNNBOF10.predict(ax_test10)\n",
    "probabilitiesPredArray  = np.asarray([float(x) for x in probabilitiesPred])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-american",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "informational-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# simple pickle results saver as a dictionary\n",
    "def resultsSave(modelName, bParams, trueValues, probaPred, history, tune_time, ftrain_time):\n",
    "    \n",
    "    saveName = modelName\n",
    "    \n",
    "    modelName = dict()\n",
    "\n",
    "    modelName['model_params'] = bParams\n",
    "\n",
    "    modelName['model_proba'] = (trueValues, probaPred)\n",
    "    \n",
    "    modelName['tf_history'] = history\n",
    "    \n",
    "    modelName['tune_train_time'] = (tune_time, ftrain_time)\n",
    "\n",
    "    filename = 'Results/'+saveName\n",
    "    \n",
    "    with open(filename, 'wb')as fp:\n",
    "        \n",
    "        pickle.dump(modelName, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "steady-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveLoadResults.resultsSave(modelName = 'RNNBOF10', \n",
    "                          bParams = best, \n",
    "                          trueValues = ay_test10, \n",
    "                          probaPred = probabilitiesPredArray, \n",
    "                          history = history.params, \n",
    "                          tune_time = tune_time_taken, \n",
    "                          ftrain_time = final_train_time_taken)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
